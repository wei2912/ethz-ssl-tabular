@inproceedings{akiba2019optuna,
  title = {Optuna: {{A Next-generation Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  series = {{{KDD}} '19},
  pages = {2623--2631},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3292500.3330701},
  urldate = {2023-08-10},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  isbn = {978-1-4503-6201-6},
  keywords = {Bayesian optimization,black-box optimization,hyperparameter optimization,machine learning system}
}

@misc{amini2023selftraining,
  title = {Self-{{Training}}: {{A Survey}}},
  shorttitle = {Self-{{Training}}},
  author = {Amini, Massih-Reza and Feofanov, Vasilii and Pauletto, Loic and Devijver, Emilie and Maximov, Yury},
  year = {2023},
  month = feb,
  number = {arXiv:2202.12040},
  eprint = {2202.12040},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-07-07},
  abstract = {Semi-supervised algorithms aim to learn prediction functions from a small set of labeled observations and a large set of unlabeled observations. Because this framework is relevant in many applications, they have received a lot of interest in both academia and industry. Among the existing techniques, self-training methods have undoubtedly attracted greater attention in recent years. These models are designed to find the decision boundary on low density regions without making additional assumptions about the data distribution, and use the unsigned output score of a learned classifier, or its margin, as an indicator of confidence. The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. The pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set. In this paper, we present self-training methods for binary and multiclass classification; as well as their variants and two related approaches, namely consistency-based approaches and transductive learning. We examine the impact of significant self-training features on various methods, using different general and image classification benchmarks, and we discuss our ideas for future research in self-training. To the best of our knowledge, this is the first thorough and complete survey on this subject.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@misc{bahri2022scarf,
  title = {{{SCARF}}: {{Self-Supervised Contrastive Learning}} Using {{Random Feature Corruption}}},
  shorttitle = {{{SCARF}}},
  author = {Bahri, Dara and Jiang, Heinrich and Tay, Yi and Metzler, Donald},
  year = {2022},
  month = mar,
  number = {arXiv:2106.15147},
  eprint = {2106.15147},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-24},
  abstract = {Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domainspecific and little has been done to leverage this technique on real-world tabular datasets. We propose SCARF, a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, SCARF not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. We show that SCARF complements existing strategies and outperforms alternatives like autoencoders. We conduct comprehensive ablations, detailing the importance of a range of factors.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\6ZRR4H92\\Bahri et al. - 2022 - SCARF Self-Supervised Contrastive Learning using .pdf}
}

@misc{balestriero2023cookbook,
  title = {A {{Cookbook}} of {{Self-Supervised Learning}}},
  author = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
  year = {2023},
  month = jun,
  number = {arXiv:2304.12210},
  eprint = {2304.12210},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-07-06},
  abstract = {Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\JAUBRT4W\\Balestriero et al. - 2023 - A Cookbook of Self-Supervised Learning.pdf}
}

@misc{cascante-bonilla2020curriculum,
  title = {Curriculum {{Labeling}}: {{Revisiting Pseudo-Labeling}} for {{Semi-Supervised Learning}}},
  shorttitle = {Curriculum {{Labeling}}},
  author = {{Cascante-Bonilla}, Paola and Tan, Fuwen and Qi, Yanjun and Ordonez, Vicente},
  year = {2020},
  month = dec,
  number = {arXiv:2001.06001},
  eprint = {2001.06001},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.06001},
  urldate = {2023-07-19},
  abstract = {In this paper we revisit the idea of pseudo-labeling in the context of semi-supervised learning where a learning algorithm has access to a small set of labeled samples and a large set of unlabeled samples. Pseudo-labeling works by applying pseudo-labels to samples in the unlabeled set by using a model trained on the combination of the labeled samples and any previously pseudo-labeled samples, and iteratively repeating this process in a self-training cycle. Current methods seem to have abandoned this approach in favor of consistency regularization methods that train models under a combination of different styles of self-supervised losses on the unlabeled samples and standard supervised losses on the labeled samples. We empirically demonstrate that pseudo-labeling can in fact be competitive with the state-of-the-art, while being more resilient to out-of-distribution samples in the unlabeled set. We identify two key factors that allow pseudo-labeling to achieve such remarkable results (1) applying curriculum learning principles and (2) avoiding concept drift by restarting model parameters before each self-training cycle. We obtain 94.91\% accuracy on CIFAR-10 using only 4,000 labeled samples, and 68.87\% top-1 accuracy on Imagenet-ILSVRC using only 10\% of the labeled samples. The code is available at https://github.com/uvavision/Curriculum-Labeling},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\LMLKMW2P\\2001.html}
}

@inproceedings{chen2016xgboost,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  eprint = {1603.02754},
  primaryclass = {cs},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  urldate = {2023-07-04},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\INSP5ED3\\1603.html}
}

@misc{chen2020simple,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  number = {arXiv:2002.05709},
  eprint = {2002.05709},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-06-22},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\TZKJDSJ5\\2002.05709.pdf}
}

@article{chen2022debiased,
  title = {Debiased {{Self-Training}} for {{Semi-Supervised Learning}}},
  author = {Chen, Baixu and Jiang, Junguang and Wang, Ximei and Wan, Pengfei and Wang, Jianmin and Long, Mingsheng},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {32424--32437},
  urldate = {2023-07-20},
  langid = {english}
}

@article{chitlangiaself,
  title = {Self {{Supervised Pre-training}} for {{Large Scale Tabular Data}}},
  author = {Chitlangia, Sharad and Muralidhar, Anand and Agarwal, Rajat},
  abstract = {In this paper, we tackle the problem of self supervised pre-training of deep neural networks for large scale tabular data in online advertising. Self supervised learning has recently been very effective for pre-training representations in domains such as vision, natural language processing, etc. But unlike these, designing self supervised learning tasks for tabular data is inherently challenging. Tabular data can consist of various types of data with high cardinality and range of feature values especially in a large scale real world setting. To that end, we propose a self supervised pre-training strategy that utilizes Manifold Mixup to produce data augmentations for tabular data and perform reconstruction on these augmentations using noise contrastive estimation and mean absolute error losses, both of which are particularly suitable for large scale tabular data. We demonstrate its efficacy by evaluating on the problem of click fraud detection on ads to obtain a 9\% relative improvement on robot detection metrics over a supervised learning baseline and 4\% over a contrastive learning experiment.},
  langid = {english}
}

@misc{darabi2021contrastive,
  title = {Contrastive {{Mixup}}: {{Self-}} and {{Semi-Supervised}} Learning for {{Tabular Domain}}},
  shorttitle = {Contrastive {{Mixup}}},
  author = {Darabi, Sajad and Fazeli, Shayan and Pazoki, Ali and Sankararaman, Sriram and Sarrafzadeh, Majid},
  year = {2021},
  month = aug,
  number = {arXiv:2108.12296},
  eprint = {2108.12296},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.12296},
  urldate = {2023-07-28},
  abstract = {Recent literature in self-supervised has demonstrated significant progress in closing the gap between supervised and unsupervised methods in the image and text domains. These methods rely on domain-specific augmentations that are not directly amenable to the tabular domain. Instead, we introduce Contrastive Mixup, a semi-supervised learning framework for tabular data and demonstrate its effectiveness in limited annotated data settings. Our proposed method leverages Mixup-based augmentation under the manifold assumption by mapping samples to a low dimensional latent space and encourage interpolated samples to have high a similarity within the same labeled class. Unlabeled samples are additionally employed via a transductive label propagation method to further enrich the set of similar and dissimilar pairs that can be used in the contrastive loss term. We demonstrate the effectiveness of the proposed framework on public tabular datasets and real-world clinical datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\4HF92N7I\\2108.html}
}

@article{elhage2021mathematical,
  title = {A Mathematical Framework for Transformer Circuits},
  author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and {Hatfield-Dodds}, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year = {2021},
  journal = {Transformer Circuits Thread}
}

@misc{englesson2021consistency,
  title = {Consistency {{Regularization Can Improve Robustness}} to {{Label Noise}}},
  author = {Englesson, Erik and Azizpour, Hossein},
  year = {2021},
  month = oct,
  number = {arXiv:2110.01242},
  eprint = {2110.01242},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-07-07},
  abstract = {Consistency regularization is a commonly-used technique for semi-supervised and self-supervised learning. It is an auxiliary objective function that encourages the prediction of the network to be similar in the vicinity of the observed training samples. Hendrycks et al. (2020) have recently shown such regularization naturally brings testtime robustness to corrupted data and helps with calibration. This paper empirically studies the relevance of consistency regularization for trainingtime robustness to noisy labels. First, we make two interesting and useful observations regarding the consistency of networks trained with the standard cross entropy loss on noisy datasets which are: (i) networks trained on noisy data have lower consistency than those trained on clean data, and (ii) the consistency reduces more significantly around noisy-labelled training data points than correctly-labelled ones. Then, we show that a simple loss function that encourages consistency improves the robustness of the models to label noise on both synthetic (CIFAR-10, CIFAR-100) and real-world (WebVision) noise as well as different noise rates and types and achieves state-ofthe-art results.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{fazakis2019selftrained,
  title = {Self-Trained {{eXtreme Gradient Boosting Trees}}},
  booktitle = {2019 10th {{International Conference}} on {{Information}}, {{Intelligence}}, {{Systems}} and {{Applications}} ({{IISA}})},
  author = {Fazakis, Nikos and Kostopoulos, Georgios and Karlos, Stamatis and Kotsiantis, Sotiris and Sgarbas, Kyriakos},
  year = {2019},
  month = jul,
  pages = {1--6},
  doi = {10.1109/IISA.2019.8900737},
  abstract = {Semi-Supervised Learning (SSL) is an ever-growing research area offering a powerful set of methods, either single or multi-view, for exploiting both labeled and unlabeled instances in the most effective manner. Self-training is a representative SSL algorithm which has been efficiently implemented for solving several classification problems in a wide range of scientific fields. Moreover, self-training has served as the base for the development of several self-labeled methods. In addition, gradient boosting is an advanced machine learning technique, a boosting algorithm for both classification and regression problems, which produces a predictive model in the form of decision trees. In this context, the principal objective of this paper is to put forward an improved self-training algorithm for classification tasks utilizing the efficacy of eXtreme Gradient Boosting (XGBoost) trees in a self-labeled scheme in order to build a highly accurate and robust classification model. A number of experiments on benchmark datasets were executed demonstrating the superiority of the proposed method over representative semi-supervised methods, as statistically verified by the Friedman non-parametric test.},
  keywords = {Boosting,Classification algorithms,extreme gradient boosting trees,Machine learning algorithms,Prediction algorithms,Predictive models,self-training,Semi-supervised learning,Task analysis,Training},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\ZUFCTNZU\\8900737.html}
}

@article{garrido2022duality,
  title = {On the Duality between Contrastive and Non-Contrastive Self-Supervised Learning},
  author = {Garrido, Quentin and Chen, Yubei and Bardes, Adrien and Najman, Laurent and Lecun, Yann},
  year = {2022},
  eprint = {2206.02574},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2206.02574},
  urldate = {2023-07-07},
  abstract = {Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and non-contrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of self-supervised learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\2RI6XJH7\\2206.html}
}

@misc{gorishniy2021revisiting,
  title = {Revisiting {{Deep Learning Models}} for {{Tabular Data}}},
  author = {Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
  year = {2021},
  month = nov,
  number = {arXiv:2106.11959},
  eprint = {2106.11959},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.11959},
  urldate = {2023-07-19},
  abstract = {The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems. In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks. Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols. We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\TBTYGG4T\\2106.html}
}

@misc{gorishniy2023tabr,
  title = {{{TabR}}: {{Unlocking}} the {{Power}} of {{Retrieval-Augmented Tabular Deep Learning}}},
  shorttitle = {{{TabR}}},
  author = {Gorishniy, Yury and Rubachev, Ivan and Kartashev, Nikolay and Shlenskii, Daniil and Kotelnikov, Akim and Babenko, Artem},
  year = {2023},
  month = jul,
  number = {arXiv:2307.14338},
  eprint = {2307.14338},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.14338},
  urldate = {2023-07-28},
  abstract = {Deep learning (DL) models for tabular data problems are receiving increasingly more attention, while the algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution. Following the recent trends in other domains, such as natural language processing and computer vision, several retrieval-augmented tabular DL models have been recently proposed. For a given target object, a retrieval-based model retrieves other relevant objects, such as the nearest neighbors, from the available (training) data and uses their features or even labels to make a better prediction. However, we show that the existing retrieval-based tabular DL solutions provide only minor, if any, benefits over the properly tuned simple retrieval-free baselines. Thus, it remains unclear whether the retrieval-based approach is a worthy direction for tabular DL. In this work, we give a strong positive answer to this question. We start by incrementally augmenting a simple feed-forward architecture with an attention-like retrieval component similar to those of many (tabular) retrieval-based models. Then, we highlight several details of the attention mechanism that turn out to have a massive impact on the performance on tabular data problems, but that were not explored in prior work. As a result, we design TabR -- a simple retrieval-based tabular DL model which, on a set of public benchmarks, demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed ``GBDT-friendly'' benchmark (see the first figure).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\59ECIVNH\\2307.html}
}

@misc{grill2020bootstrap,
  title = {Bootstrap Your Own Latent: {{A}} New Approach to Self-Supervised {{Learning}}},
  shorttitle = {Bootstrap Your Own Latent},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan Daniel and Gheshlaghi Azar, Mohammad and Piot, Bilal and Kavukcuoglu, Koray and Munos, R{\'e}mi and Valko, Michal},
  year = {2020},
  month = jun,
  journal = {arXiv e-prints},
  doi = {10.48550/arXiv.2006.07733},
  urldate = {2023-07-07},
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3\textbackslash\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6\textbackslash\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {ADS Bibcode: 2020arXiv200607733G}
}

@misc{grinsztajn2022why,
  title = {Why Do Tree-Based Models Still Outperform Deep Learning on Tabular Data?},
  author = {Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  year = {2022},
  month = jul,
  number = {arXiv:2207.08815},
  eprint = {2207.08815},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.08815},
  urldate = {2023-07-05},
  abstract = {While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data (\$\textbackslash sim\$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs). This leads to a series of challenges which should guide researchers aiming to build tabular-specific NNs: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\37AFUZRZ\\Grinsztajn et al. - 2022 - Why do tree-based models still outperform deep lea.pdf;C\:\\Users\\Wei En\\Zotero\\storage\\UMZVUJWB\\2207.html}
}

@inproceedings{kemp2003semisupervised,
  title = {Semi-{{Supervised Learning}} with {{Trees}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kemp, Charles and Griffiths, Thomas and Stromsten, Sean and Tenenbaum, Joshua},
  year = {2003},
  volume = {16},
  publisher = {{MIT Press}},
  urldate = {2023-08-10},
  abstract = {We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain. The tree (or a distribution over trees) may be inferred using the unlabeled data. A prior over concepts generated by a mutation process on the inferred tree(s) allows efficient computation of the optimal Bayesian classification func- tion from the labeled examples. We test our approach on eight real-world datasets.}
}

@article{lee2013pseudolabel,
  title = {Pseudo-{{Label}} : {{The Simple}} and {{Efficient Semi-Supervised Learning Method}} for {{Deep Neural Networks}}},
  author = {Lee, Dong-Hyun},
  year = {2013},
  abstract = {We propose the simple and efficient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For unlabeled data, Pseudo-Label s, just picking up the class which has the maximum predicted probability, are used as if they were true labels. This is in effect equivalent to Entropy Regularization. It favors a low-density separation between classes, a commonly assumed prior for semi-supervised learning. With Denoising Auto-Encoder and Dropout, this simple method outperforms conventional methods for semi-supervised learning with very small labeled data on the MNIST handwritten digit dataset.},
  langid = {english},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\5FK5FIVB\\Lee - Pseudo-Label  The Simple and Efficient Semi-Super.pdf}
}

@article{levatic2017semisupervised,
  title = {Semi-Supervised Classification Trees},
  author = {Levati{\'c}, Jurica and Ceci, Michelangelo and Kocev, Dragi and D{\v z}eroski, Sa{\v s}o},
  year = {2017},
  month = dec,
  journal = {Journal of Intelligent Information Systems},
  volume = {49},
  number = {3},
  pages = {461--486},
  issn = {1573-7675},
  doi = {10.1007/s10844-017-0457-4},
  urldate = {2023-08-10},
  abstract = {In many real-life problems, obtaining labelled data can be a very expensive and laborious task, while unlabeled data can be abundant. The availability of labeled data can seriously limit the performance of supervised learning methods. Here, we propose a semi-supervised classification tree induction algorithm that can exploit both the labelled and unlabeled data, while preserving all of the appealing characteristics of standard supervised decision trees: being non-parametric, efficient, having good predictive performance and producing readily interpretable models. Moreover, we further improve their predictive performance by using them as base predictive models in random forests. We performed an extensive empirical evaluation on 12 binary and 12 multi-class classification datasets. The results showed that the proposed methods improve the predictive performance of their supervised counterparts. Moreover, we show that, in cases with limited availability of labeled data, the semi-supervised decision trees often yield models that are smaller and easier to interpret than supervised decision trees.},
  langid = {english},
  keywords = {Binary classification,Decision trees,Multi-class classification,Random forests,Semi-supervised learning}
}

@misc{levin2022transfer,
  title = {Transfer {{Learning}} with {{Deep Tabular Models}}},
  author = {Levin, Roman and Cherepanova, Valeriia and Schwarzschild, Avi and Bansal, Arpit and Bruss, C. Bayan and Goldstein, Tom and Wilson, Andrew Gordon and Goldblum, Micah},
  year = {2022},
  month = jun,
  number = {arXiv:2206.15306},
  eprint = {2206.15306},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.15306},
  urldate = {2023-07-07},
  abstract = {Recent work on deep learning for tabular data demonstrates the strong performance of deep tabular models, often bridging the gap between gradient boosted decision trees and neural networks. Accuracy aside, a major advantage of neural models is that they learn reusable features and are easily fine-tuned in new domains. This property is often exploited in computer vision and natural language applications, where transfer learning is indispensable when task-specific training data is scarce. In this work, we demonstrate that upstream data gives tabular neural networks a decisive advantage over widely used GBDT models. We propose a realistic medical diagnosis benchmark for tabular transfer learning, and we present a how-to guide for using upstream data to boost performance with a variety of tabular neural network architectures. Finally, we propose a pseudo-feature method for cases where the upstream and downstream feature sets differ, a tabular-specific problem widespread in real-world applications. Our code is available at https://github.com/LevinRoman/tabular-transfer-learning .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\AGSG7P6G\\2206.html}
}

@misc{miyato2018virtual,
  title = {Virtual {{Adversarial Training}}: {{A Regularization Method}} for {{Supervised}} and {{Semi-Supervised Learning}}},
  shorttitle = {Virtual {{Adversarial Training}}},
  author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
  year = {2018},
  month = jun,
  number = {arXiv:1704.03976},
  eprint = {1704.03976},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1704.03976},
  urldate = {2023-08-07},
  abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is defined as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method defines the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only "virtually" adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\Y44678IF\\Miyato et al. - 2018 - Virtual Adversarial Training A Regularization Met.pdf;C\:\\Users\\Wei En\\Zotero\\storage\\L6YF6AZI\\1704.html}
}

@article{natekin2013gradient,
  title = {Gradient Boosting Machines, a Tutorial},
  author = {Natekin, Alexey and Knoll, Alois},
  year = {2013},
  journal = {Frontiers in Neurorobotics},
  volume = {7},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2013.00021},
  urldate = {2023-07-15},
  abstract = {Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods with a strong focus on machine learning aspects of modeling. A theoretical information is complemented with descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. Three practical examples of gradient boosting applications are presented and comprehensively analyzed.},
  langid = {english},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\2AJLWD8X\\Natekin and Knoll - 2013 - Gradient boosting machines, a tutorial.pdf}
}

@inproceedings{ng2004feature,
  title = {Feature Selection, {{{\emph{L}}}} {\textsubscript{1}} vs. {{{\emph{L}}}} {\textsubscript{2}} Regularization, and Rotational Invariance},
  booktitle = {Twenty-First International Conference on {{Machine}} Learning  - {{ICML}} '04},
  author = {Ng, Andrew Y.},
  year = {2004},
  pages = {78},
  publisher = {{ACM Press}},
  address = {{Banff, Alberta, Canada}},
  doi = {10.1145/1015330.1015435},
  urldate = {2023-07-07},
  abstract = {We consider supervised learning in the presence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logistic regression, we show that using L1 regularization of the parameters, the sample complexity (i.e., the number of training examples required to learn ``well,'') grows only logarithmically in the number of irrelevant features. This logarithmic rate matches the best known bounds for feature selection, and indicates that L1 regularized logistic regression can be effective even if there are exponentially many irrelevant features as there are training examples. We also give a lowerbound showing that any rotationally invariant algorithm\textemdash including logistic regression with L2 regularization, SVMs, and neural networks trained by backpropagation\textemdash has a worst case sample complexity that grows at least linearly in the number of irrelevant features.},
  langid = {english},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\KHHQUUIA\\Ng - 2004 - Feature selection, L 1 vs. L.pdf}
}

@misc{oord2019representation,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2019},
  month = jan,
  number = {arXiv:1807.03748},
  eprint = {1807.03748},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-06-21},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\TVNEV8TA\\Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf}
}

@inproceedings{paszke2019pytorch,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-08-10},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.}
}

@article{pedregosa2011scikitlearn,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  issn = {1533-7928},
  urldate = {2023-08-10},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.}
}

@misc{pokle2022contrasting,
  title = {Contrasting the Landscape of Contrastive and Non-Contrastive Learning},
  author = {Pokle, Ashwini and Tian, Jinjin and Li, Yuchen and Risteski, Andrej},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15702},
  eprint = {2203.15702},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.15702},
  urldate = {2023-07-07},
  abstract = {A lot of recent advances in unsupervised feature learning are based on designing features which are invariant under semantic data augmentations. A common way to do this is contrastive learning, which uses positive and negative samples. Some recent works however have shown promising results for non-contrastive learning, which does not require negative samples. However, the non-contrastive losses have obvious "collapsed" minima, in which the encoders output a constant feature embedding, independent of the input. A folk conjecture is that so long as these collapsed solutions are avoided, the produced feature representations should be good. In our paper, we cast doubt on this story: we show through theoretical results and controlled experiments that even on simple data models, non-contrastive losses have a preponderance of non-collapsed bad minima. Moreover, we show that the training process does not avoid these minima.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\NFIIRLH3\\2203.html}
}

@misc{raschka0000short,
  title = {A {{Short Chronology Of Deep Learning For Tabular Data}}},
  author = {Raschka, Sebastian},
  year = {07:00:00 +0000},
  journal = {Sebastian Raschka, PhD},
  urldate = {2023-06-24},
  abstract = {Occasionally, I share research papers proposing new deep learning approaches for tabular data on social media, which is typically an excellent discussion sta...},
  howpublished = {https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html},
  langid = {english},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\ILCXF2C7\\deep-learning-for-tabular-data.html}
}

@misc{shwartz-ziv2021tabular,
  title = {Tabular {{Data}}: {{Deep Learning}} Is {{Not All You Need}}},
  shorttitle = {Tabular {{Data}}},
  author = {{Shwartz-Ziv}, Ravid and Armon, Amitai},
  year = {2021},
  month = nov,
  number = {arXiv:2106.03253},
  eprint = {2106.03253},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.03253},
  urldate = {2023-06-24},
  abstract = {A key element in solving real-life data science problems is selecting the types of models to use. Tree ensemble models (such as XGBoost) are usually recommended for classification and regression problems with tabular data. However, several deep learning models for tabular data have recently been proposed, claiming to outperform XGBoost for some use cases. This paper explores whether these deep models should be a recommended option for tabular data by rigorously comparing the new deep models to XGBoost on various datasets. In addition to systematically comparing their performance, we consider the tuning and computation they require. Our study shows that XGBoost outperforms these deep models across the datasets, including the datasets used in the papers that proposed the deep models. We also demonstrate that XGBoost requires much less tuning. On the positive side, we show that an ensemble of deep models and XGBoost performs better on these datasets than XGBoost alone.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\RYUN57BC\\Shwartz-Ziv and Armon - 2021 - Tabular Data Deep Learning is Not All You Need.pdf;C\:\\Users\\Wei En\\Zotero\\storage\\J5JG6U39\\2106.html}
}

@misc{somepalli2021saint,
  title = {{{SAINT}}: {{Improved Neural Networks}} for {{Tabular Data}} via {{Row Attention}} and {{Contrastive Pre-Training}}},
  shorttitle = {{{SAINT}}},
  author = {Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C. Bayan and Goldstein, Tom},
  year = {2021},
  month = jun,
  number = {arXiv:2106.01342},
  eprint = {2106.01342},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.01342},
  urldate = {2023-07-07},
  abstract = {Tabular data underpins numerous high-impact applications of machine learning from fraud detection to genomics and healthcare. Classical approaches to solving tabular problems, such as gradient boosting and random forests, are widely used by practitioners. However, recent deep learning methods have achieved a degree of performance competitive with popular techniques. We devise a hybrid deep learning approach to solving tabular data problems. Our method, SAINT, performs attention over both rows and columns, and it includes an enhanced embedding method. We also study a new contrastive self-supervised pre-training method for use when labels are scarce. SAINT consistently improves performance over previous deep learning methods, and it even outperforms gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on average over a variety of benchmark tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\XDYA4DHX\\2106.html}
}

@article{tanha2017semisupervised,
  title = {Semi-Supervised Self-Training for Decision Tree Classifiers},
  author = {Tanha, Jafar and {van Someren}, Maarten and Afsarmanesh, Hamideh},
  year = {2017},
  month = feb,
  journal = {International Journal of Machine Learning and Cybernetics},
  volume = {8},
  number = {1},
  pages = {355--370},
  issn = {1868-808X},
  doi = {10.1007/s13042-015-0328-7},
  urldate = {2023-07-07},
  abstract = {We consider semi-supervised learning, learning task from both labeled and unlabeled instances and in particular, self-training with decision tree learners as base learners. We show that standard decision tree learning as the base learner cannot be effective in a self-training algorithm to semi-supervised learning. The main reason is that the basic decision tree learner does not produce reliable probability estimation to its predictions. Therefore, it cannot be a proper selection criterion in self-training. We consider the effect of several modifications to the basic decision tree learner that produce better probability estimation than using the distributions at the leaves of the tree. We show that these modifications do not produce better performance when used on the labeled data only, but they do benefit more from the unlabeled data in self-training. The modifications that we consider are Naive Bayes Tree, a combination of No-pruning and Laplace correction, grafting, and using a distance-based measure. We then extend this improvement to algorithms for ensembles of decision trees and we show that the ensemble learner gives an extra improvement over the adapted decision tree learners.},
  langid = {english},
  keywords = {Decision tree learning,Ensemble learning,Self-training,Semi-supervised learning}
}

@misc{wang2022understanding,
  title = {Understanding {{Contrastive Representation Learning}} through {{Alignment}} and {{Uniformity}} on the {{Hypersphere}}},
  author = {Wang, Tongzhou and Isola, Phillip},
  year = {2022},
  month = aug,
  number = {arXiv:2005.10242},
  eprint = {2005.10242},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.10242},
  urldate = {2023-07-05},
  abstract = {Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Remarkably, directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning. Project Page: https://tongzhouwang.info/hypersphere Code: https://github.com/SsnL/align\_uniform , https://github.com/SsnL/moco\_align\_uniform},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\YQ3Q9YTU\\2005.html}
}

@misc{wei2022theoretical,
  title = {Theoretical {{Analysis}} of {{Self-Training}} with {{Deep Networks}} on {{Unlabeled Data}}},
  author = {Wei, Colin and Shen, Kendrick and Chen, Yining and Ma, Tengyu},
  year = {2022},
  month = apr,
  number = {arXiv:2010.03622},
  eprint = {2010.03622},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.03622},
  urldate = {2023-06-24},
  abstract = {Self-training algorithms, which train a model to fit pseudolabels predicted by another previously-learned model, have been very successful for learning with unlabeled data using neural networks. However, the current theoretical understanding of self-training only applies to linear models. This work provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervised learning. At the core of our analysis is a simple but realistic "expansion" assumption, which states that a low probability subset of the data must expand to a neighborhood with large probability relative to the subset. We also assume that neighborhoods of examples in different classes have minimal overlap. We prove that under these assumptions, the minimizers of population objectives based on self-training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. By using off-the-shelf generalization bounds, we immediately convert this result to sample complexity guarantees for neural nets that are polynomial in the margin and Lipschitzness. Our results help explain the empirical successes of recently proposed self-training algorithms which use input consistency regularization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\X7P8IVC5\\Wei et al. - 2022 - Theoretical Analysis of Self-Training with Deep Ne.pdf;C\:\\Users\\Wei En\\Zotero\\storage\\EQN39VJM\\2010.html}
}

@inproceedings{xie2020selftraining,
  title = {Self-{{Training With Noisy Student Improves ImageNet Classification}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
  year = {2020},
  pages = {10687--10698},
  urldate = {2023-07-07}
}

@inproceedings{yoon2020vime,
  title = {{{VIME}}: {{Extending}} the {{Success}} of {{Self-}} and {{Semi-supervised Learning}} to {{Tabular Domain}}},
  shorttitle = {{{VIME}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yoon, Jinsung and Zhang, Yao and Jordon, James and {van der Schaar}, Mihaela},
  year = {2020},
  volume = {33},
  pages = {11033--11043},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-07-07},
  abstract = {Self- and semi-supervised learning frameworks have made significant progress in training machine learning models with limited labeled data in image and language domains. These methods heavily rely on the unique structure in the domain datasets (such as spatial relationships in images or semantic relationships in language). They are not adaptable to general tabular data which does not have the same explicit structure as image and language data. In this paper, we fill this gap by proposing novel self- and semi-supervised learning frameworks for tabular data, which we refer to collectively as VIME (Value Imputation and Mask Estimation). We create a novel pretext task of estimating mask vectors from corrupted tabular data in addition to the reconstruction pretext task for self-supervised learning. We also introduce a novel tabular data augmentation method for self- and semi-supervised learning frameworks. In experiments, we evaluate the proposed framework in multiple tabular datasets from various application domains, such as genomics and clinical data. VIME exceeds state-of-the-art performance in comparison to the existing baseline methods.}
}

@misc{zoph2020rethinking,
  title = {Rethinking {{Pre-training}} and {{Self-training}}},
  author = {Zoph, Barret and Ghiasi, Golnaz and Lin, Tsung-Yi and Cui, Yin and Liu, Hanxiao and Cubuk, Ekin D. and Le, Quoc V.},
  year = {2020},
  month = nov,
  number = {arXiv:2006.06882},
  eprint = {2006.06882},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.06882},
  urldate = {2023-07-20},
  abstract = {Pre-training is a dominant paradigm in computer vision. For example, supervised ImageNet pre-training is commonly used to initialize the backbones of object detection and segmentation models. He et al., however, show a surprising result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from +1.3 to +3.4AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help significantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 54.3AP, an improvement of +1.5AP over the strongest SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improvement of +1.5\% mIOU over the previous state-of-the-art result by DeepLabv3+.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Wei En\\Zotero\\storage\\AHAI6LTC\\2006.html}
}
